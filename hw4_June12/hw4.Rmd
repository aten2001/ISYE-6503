---
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r}
library("GGally")
library("DAAG")
library(randomForest)
library(pROC)
set.seed(1234)
```


#Question 9.1
Using the same crime data set uscrime.txt as in Question 8.2, apply Principal Component Analysis
and then create a regression model using the first few principal components. Specify your new model in
terms of the original variables (not the principal components), and compare its quality to that of your
solution to Question 8.2. You can use the R function prcomp for PCA. 

```{r}
crime<- read.table("http://www.statsci.org/data/general/uscrime.txt",header=TRUE)
head(crime)
```

#Check out if there are correlations between the predictors
```{r cache=TRUE}
names(crime)
ggpairs(crime,columns=c("M","So","Ed","Po1","Po2","LF","M.F","Pop","NW","U1","U2","Wealth","Ineq","Prob","Time"))
```

#There are correlations between Po1 vs Po2, Wealth vs Ed/Po1/Po2/Ineq , so PCA is a good choose.

# remove the response variable (it's in the 16th column)
```{r}
vars<-crime[-16]
pca<-prcomp(vars, scale = TRUE)
summary(pca)
```

#get the eigenvector of the matrix
```{r}
eigen<-pca$rotation
```

#Use the screenplot to plot the variance of each princpal component
```{r}
screeplot(pca,type="line",col="black")
```

#get the first 4 pc
```{r}
pc<-pca$x[,1:4]
```
#fit a linear regression model with the these 4 pc
```{r}
crimepc<-as.data.frame(cbind(pc,crime$Crime))
modelpca<-lm(V5~.,crimepc)
summary(modelpca)
```

#Get the parameters for the original model,scaled
```{r}
beta0<-modelpca$coefficients[1]
betas<-modelpca$coefficients[2:5]
```

#coefficents equals beta times eigenvector matrix
```{r}
alpha<-eigen[,1:4] %*% betas
alpha

mean <- sapply(vars, mean)
sd <- sapply(vars, sd)
```

#get the un-scaled coefficents for each input
```{r}
alpha_org<- alpha/sd
```

#get the un-scaled intercept
```{r}
beta_org <-beta0-sum(alpha* mean/sd)

point<-data.frame(
        M = 14.0,
        So = 0,
        Ed = 10.0,
        Po1 = 12.0,
        Po2 = 15.5,
        LF = 0.640,
        M.F = 94.0,
        Pop = 150,
        NW = 1.1,
        U1 = 0.120,
        U2 = 3.6,
        Wealth = 3200,
        Ineq = 20.1,
        Prob = 0.04,
        Time = 39.0
)

predict<-beta_org+sum(alpha_org*point)
```

#cross validate the model
```{r}
rate<-crime[,16]
PClist <- as.data.frame(pca$x[, 1:4])
PC<-cbind(rate, PClist)
model2 <- lm(rate ~ ., PC)
cv <-cv.lm(PC, model2, m = 5)
mn<- mean(rate)
R2 <- 1 - attr(cv, "ms") * nrow(crime) / sum((rate - mn) ^ 2)
R2
```

In conclusion, the model generated by the PCA method is: 
Crime=1666.485-16.9307630*M+21.3436771*So+12.8297238*Ed +21.3521593*Po1+23.0883154*Po2
-346.5657125*LF-8.2930969*M.F+1.0462155*Pop+1.5009941*NW-1509.9345216*U1+1.6883674*U2 
+0.0400119*Wealth-6.9020218*Ineq+144.9492678*Prob-0.9330765*Time

The adjusted R-square of this model is 0.2433, cross-validated R-square is 0.0392,which is pretty low. The crime rate for the city with given data is 1112.678 
Compared to my model for question 8.2, with predictors:M, Ed, Po1, M.F, U1,U2,Ineq,Prob, and R-square: 0.7444, and crime rate: 1038.296,
My conclustion is, adding more principle compenents to the model may be helpful (currenlty we used first 4 compnents). Although PCA method addressed for the correlations between the predcitors and ranked the coordinates by importance. It didn't address for the overfitting, which is a big issue in our data. 

#Question 10.1
Using the same crime data set uscrime.txt as in Questions 8.2 and 9.1, find the best model you can using
(a) a regression tree model, and
(b) a random forest model. 


#(b) a random forest model

#Set the number of predictors at each split of the tree to be 4 (mtry=4),which is calculated based 1+log(n)=1+log(16)=4
```{r}
rf <- randomForest(Crime ~ ., data=crime, mtry=4,importance=TRUE, na.action=na.omit)
rf
```

# Plot of actual vs. predicted crime values
```{r}
yhatrf <- predict(rf)
plot(crime$Crime, yhatrf)
```

#Calculate sum of square error-resdiduals  
```{r}
SSres <- sum((yhatrf-crime$Crime)^2)
```

#Calculate sum of square error-total and R-squared  
```{r}
SStot <- sum((crime$Crime - mean(crime$Crime))^2)
rs <- 1 - SSres/SStot
rs
```

#This model is slightly better than the previous model.But it is not a real model. Iit is the average of all the different trees, which is better than just
#one tree.

#variable importance
```{r}
round(importance(rf), 2)
varImpPlot(rf)
```
#We can see that Po1 is the most important variable among all the predictors.It also suggest the overfitting if we use all the predictors in the model.

#Question 10.2
#Describe a situation or problem from your job, everyday life, current events, etc., for which a logistic
#regression model would be appropriate. List some (up to 5) predictors that you might use.
#The likelyhood of the applicant be admiited to the graduate school 
#Predictors: 
#GRE score, GPA from the undergraduate,have related research exprience or not, whether or not the undergraduate major is related to the program applying for

#Question 10.3
#1. Using the GermanCredit data set germancredit.txt, use logistic
#regression to find a good predictive model for whether credit applicants are good credit risks or
#not. Show your model (factors used and their coefficients), the software output, and the quality
#of fit. 
```{r}
credit<- read.table("germancredit.txt",header=FALSE)
head(credit)
str(credit)
```

#accordingly to the description, we found that V21 is the response. 1 means good, 2 means bad.
#Recode the V21 to be a 0/1 variable, instead of 1/2
```{r}
credit$V21[credit$V21==1]<-0
credit$V21[credit$V21==2]<-1
```

# Divide the data into training and test datasets.
```{r}
trainno <- sample(1:nrow(credit), size = round(nrow(credit)*0.7), replace = FALSE)
train <- credit[trainno,]
test<- credit[-trainno,]
```

#Fit the logistic model
```{r}
log<-glm(V21~.,data=train,family=binomial(link="logit"))
summary(log)
```

#keep the significant preditors under p-value=0.1,for the categorical predictors, keep them if any of the categories are significant. Then re-fit the model
```{r}
log2<-glm(V21~V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V12+V14+V16+V19+V20,data=train,family=binomial(link="logit"))
summary(log2)
```
#For the categorical variables, not all the levels are significant. So create a binary (0/1) variable for each of them: 0 for not significant, 1 for significant
```{r}
train$V1A12[train$V1=="A12"]<-1
train$V1A12[train$V1!="A12"]<-0

train$V1A13[train$V1=="A13"]<-1
train$V1A13[train$V1!="A13"]<-0

train$V1A14[train$V1=="A14"]<-1
train$V1A14[train$V1!="A14"]<-0

train$V3A34[train$V1=="A34"]<-1
train$V3A34[train$V1!="A34"]<-0

train$V4A41[train$V1=="A41"]<-1
train$V4A41[train$V1!="A41"]<-0

train$V4A42[train$V1=="A42"]<-1
train$V4A42[train$V1!="A42"]<-0

train$V4A43[train$V1=="A43"]<-1
train$V4A43[train$V1!="A43"]<-0

train$V6A65[train$V1=="A65"]<-1
train$V6A65[train$V1!="A65"]<-0

train$V7A74[train$V1=="A74"]<-1
train$V7A74[train$V1!="A74"]<-0

train$V9A92[train$V1=="A92"]<-1
train$V9A92[train$V1!="A92"]<-0

train$V9A93[train$V1=="A93"]<-1
train$V9A93[train$V1!="A93"]<-0

train$V10A103[train$V1=="A103"]<-1
train$V10A103[train$V1!="A103"]<-0

train$V12A124[train$V1=="A124"]<-1
train$V12A124[train$V1!="A124"]<-0

train$V14A143[train$V1=="A143"]<-1
train$V14A143[train$V1!="A143"]<-0

train$V19A192[train$V1=="A192"]<-1
train$V19A192[train$V1!="A192"]<-0

train$V20A202[train$V1=="A202"]<-1
train$V20A202[train$V1!="A202"]<-0
```
#re-fit the model with these significant variables
```{r}
log3<-glm(V21~V1A12+V1A13+V1A14+V2+V3A34+V4A41+V4A42+V4A43+V5+V6A65+V7A74+V8+V9A92+V9A93+V10A103+V12A124+V14A143+V16+V19A192+V20A202,data=train,family=binomial(link="logit"))
summary(log3)
```
#only keep the significant terms
```{r}
log4<-glm(V21~V1A12+V1A13+V1A14+V2+V5+V8,data=train,family=binomial(link="logit"))
summary(log4)
```
#Now every term are significant, this is the final model

#Add the remained binary variables to the test dataset
```{r}
test$V1A12[test$V1=="A12"]<-1
test$V1A12[test$V1!="A12"]<-0

test$V1A13[test$V1=="A13"]<-1
test$V1A13[test$V1!="A13"]<-0

test$V1A14[test$V1=="A14"]<-1
test$V1A14[test$V1!="A14"]<-0
```

#validate the model using the test dataset
```{r}
yhatlog<-predict(log4,test,type = "response")
head(yhatlog)
```
#round the yhatlog to be 0/1 variabls
```{r}
y<- as.integer(yhatlog > 0.5)
head(y)

t <- table(y,test$V21)
t

correct<-(182+33)/300
correct

roc<-roc(test$V21,y)
```

# Plot the ROC curve
```{r}
plot(roc)
roc
```
#The model I developed is: log(p/(1-p))=-1.285e+00-5.099e-01*V1A12-1.155e+00*V1A13-2.316e+00*V1A14+2.545e-02*V2+9.637e-05*V5+1.633e-01*V8
The accuracy rate is 71.67%, AIC is 717.06,and AUC is 61.67%,which means the model will correctly classify the samples 61.67% of the times.

#2. Because the model gives a result between 0 and 1, it requires setting a threshold probability to
separate between ��good�� and ��bad�� answers. In this data set, they estimate that incorrectly
identifying a bad customer as good, is 5 times worse than incorrectly classifying a good customer
as bad. Determine a good threshold probability based on your model.

Calulating loss for tthe cost for thresholds ranging from 0.01 to 1. 

```{r}
cost <- c()
for(i in 1:100){
        y.hat<- as.integer(yhatlog > (i/100)) #0.01-100
        
        table<-as.matrix(table(y.hat,test$V21))
        
        if(nrow(table)>1) { cst1 <- table[2,1] } else { cst1 <- 0 }
        if(ncol(table)>1) { cst2 <- table[1,2] } else { cst2 <- 0 }
        cost <- c(cost, cst1+cst2*5)
}

plot(c(1:100)/100,cost,xlab = "Threshold",ylab = "Cost")

which.min(cost)
cost
```
#when threshold=0.08, we have minimum cost 187.